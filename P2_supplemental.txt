Team Members: Nischal Dinesh, Sai Abhinov Chowdary Katragadda

Q1.1:explain the implementation of ReflexAgent  in multiAgents.py and how you improved it.

Answer:
The ReflexAgent in multiAgents.py evaluates actions based on food and ghost distances:
- FoodDistances: Calculates distance from Pacman’s new position to all food pellets.
- GhostDistances: Computes distance to each ghost to avoid them.
- Evaluation: Combines:
    - Closer to food means Higher score.
    - Closer to ghosts means Lower score.

Improvement Made: Used inverse distances (1/distance) to prioritize nearby food and avoid ghosts.
                    Increased the penalty on score for ghost distance to 10 units from 1 unit, to avoid collision(death of pacman), 
                    but not so high that the pacman freezes and the game never ends. 

Q1.2:What is your value function and why do you think this estimation makes sense?

Answer: The value function is the sum of the score of current state of pacman, the multiplicative inverse of the 
        (shortest food distance + 1) and product of -10 and the multiplicative inverse of the (shortest ghost distance + 1).

        This value function makes sense, since it encourages the pacman to explore the nearest food pellets, while penalizes 
        the presence of ghost around pacman.

        To avoid the death of pacman during the game, the penalty for closer ghosts is penalized on score, but over penalizing the
        pacman for closer ghosts wouldnot do any good in this scenario.

        When the penalty for ghost distance was -10, which was chosen randomly, Pacman won all test cases, 
        but when penalty of -100 was used, the pacman was not successfull in finishing the games at all. 
    




Q2.1:Explain your algorithm. Why do you think it is working?

Answer:
The Minimax algorithm is used to find the best move for Pacman, where ghosts will play optimally to minimize Pacman’s score.
1.Pacman's Turn (Maximizing):
    - Pacman tries to maximize the evaluation score.
    - Chooses the action that leads to the best possible outcome.
2. Ghost's Turn (Minimizing):
    - Each ghost tries to minimize Pacman’s score.
    - The ghosts act in sequence.
3. Recursive Exploration:
    - The algorithm explores all possible future states up to a certain depth.
    - It evaluates each state using the evaluation function if it reaches the depth limit or a terminal state of win or lose.
4. Returns the Best Move:
    - Pacman picks the action with the maximum value.
    - Ghosts pick actions that minimize Pacman's value.

- It simulates both Pacman and ghost behaviors, assuming both play their best moves.
- It evaluates each action based on future consequences, not just the immediate result.



Q3.1:The AlphaBetaAgent minimax values should be identical to the MinimaxAgent minimax values.  Explain why.

Answer:
Because alpha beta pruning does not change the result of the minimax algorithm, it only improves the efficiency.
Both algorithms explore the same game tree and apply the same evaluation function to select the best action. 
Alpha beta pruning prunes branches that cannot possibly affect the final decision, reducing the number of states evaluated. 
However, the final chosen action and the values assigned to states will be the same as with the regular minimax algorithm. 
This is because alpha beta pruning does not ignore any optimal moves, it just avoids unnecessary exploration of worse paths.


Q3.2: explain your strategy for breaking a tie.

Answer:
Our strategy for breaking a tie is to choose randomly among the best actions when multiple actions have the same minimax value.
chosenIndex = random.choice(bestIndices)
- This prevents Pacman from getting stuck in loops.

Q4.1:Explain your Expectimax algorithm

Answer:
Expectimax algorithm is similar to Minimax, but it handles ghost moves differently. 
Instead of assuming that the ghosts play optimally to minimize Pacman's score, Expectimax assumes that the ghosts move randomly. 
- Pacman's Turn (Maximizing): Pacman still chooses the action that maximizes the evaluation score.
- Ghost's Turn (Chance Nodes): Instead of picking the minimum score, we calculate the average score across all possible ghost actions because their behavior is random.


Q5.1:explain your new evaluation function, why do you think this new evaluation function is better compared to the previous one?

Answer:
Our new evaluation function evaluates the state based on four factors:
    -Game Score: Prioritizes increasing the current score.
    -Distance to Nearest Food: Encourages Pacman to move toward the closest food pellet using 1/(distance + 1) to give higher value to closer food.
    -Distance to Nearest Ghost: Penalizes Pacman for being close to a ghost using -1/(distance + 1) to avoid danger.
    -Scared Time : Encourages Pacman to feed on Ghost when it comes closer while the special pellet is beside pacman. 


This is better because it balances food collection, ghost avoidance, and it rewards being close to food, penalizes being near active ghosts like the previous one
 but it has additional feature that enables Pacman to score bonus points by feeding ghost/ghosts while they are scared of Pacman.